| Model                                     |      | Tesseract                                                    | Kraken                                                       | Ocropus                                                      | Calamari                                                     |
| ----------------------------------------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Direction of LSTM                         |      | defined in RNN layer                                         | -s, —spec defined in RNN layer                               | —unidirectional: use only unidirectional LSTM                | ❌                                                            |
| Model Structure                           |      | —net_spec, string[ none]\[1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx256 O1c111\] | -s, —spec, test[1,48,0,1 Cr3,3,32 Do0.1,2 Mp2,2 Cr3,3,64 Do0.1,2 Mp2,2 S1(1x12)1,3 Lbx100 Do]. |                                                              | --network=cnn=40:3x3,pool=2x2,cnn=60:3x3,pool=2x2,lstm=200,dropout=0.5 The network structure Specify the network structure in a simple language. The default network consists of a stack of two CNN- and Pooling-Layers, respectively and a following LSTM layer. The network uses the default CTC-Loss implemented in Tensorflow for training and a dropout-rate of 0.5. The creation string thereto is: `cnn=40:3x3,pool=2x2,cnn=60:3x3,pool=2x2,lstm=200,dropout=0.5`. To add additional layers or remove a single layer just add or remove it in the comma separated list. Note that the order is important! |
| Parameters                                |      | same with Kraken                                             | [batch, height, width, channels]<br>1. 0 value: variable. <br>2. integer value of height or width: rescale image height or width<br>3. Channels: 1 for grayscale or B/W input, 3 for RGB color images |                                                              | --line_height LINE_HEIGHT: The line height: The height of each rescaled input file passed to the network. |
| CNN                                       |      | same with Kraken                                             | kernel size, output size, activation<br>C[{name}]\(s\|t\|r\|l\|m) \[{name}\]\<y\>,\<x\>,\<d\> |                                                              | cnn=\<d\>:\<h\>x\<w\>                                        |
| Max  Pool                                 |      | same with Kraken                                             | kernel size, stride size<br>Mp[{name}\]\<y\>,\<x\>[,<y_stride>, <x_stride>] |                                                              | pool=<h\>x<w\>                                               |
| Dropout                                   |      | ❌                                                           | Dropout probability and dimension(1D or 2D). <br>Do[{name}][\<prob\>\],[\<dim\>\] , set dim to 2 after convolutional layers |                                                              | droput=\<probability\>                                       |
| RNN                                       |      | same with Kraken                                             | LSTM or GRU cell, LSTM direction, time-axis, output size, summarize output in the requested dimension<br>(L\|G)[{name}]\(f\|b\|r)\(x\|y\) [s]\[{name}\]\<n\> | -S —hiddensize[100] LSTM state units                         | lstm=\<n\>                                                   |
| Reshape                                   |      | Different from tesseract                                     | Used to remove undesirable non-1 height before a recurrent layer. **This S layer is equivalent to the one implemented in the tensorflow implementation of VGSL, i.e. behaves differently from tesseract.**Reshape a source dimension d to a, b and distributes a into imension e, b in f. Either e or f need to be equual to d.<br>S[{name}]\<d\>(\<a\>x\<b\>)\<e\>,\<f\> |                                                              | ❌                                                            |
| Linear                                    |      | same with Kraken                                             | output size <br>01c\<s\>                                     |                                                              | ❌                                                            |
| Modify Top Layers                         |      | —append_index:[int,-1]cut the head off the network at the given index and append —net_spec network in place of cut off part | -a, --append INTEGER: remove layers before argument and then appends spec. Only work when loading an existing model |                                                              | ?                                                            |
| Loading Existing Model                    |      | --continue_from[string, none):path to previous checkpoint from which to continue training or fine tune(training checkpoint or a recognition model)<br>--stop_training[false): convert the training checkpoint in --continue_from to a recognition model<br>—convert_to_int[bool, false]: when using stop_training, convert to 8-bit integer for greater speed, with slightly less accuracy | -i, --load PATH: Load existing file to continue training     |                                                              | --weights WEIGHTS, string<br>Load network weights from the given file |
| Range of random initialization of weights |      | --weight_range[0.1]                                          |                                                              |                                                              |                                                              |
| Learning    Rate                          |      | ❌                                                     | -r, --lrate FLOAT      [0.002]                               | -r LRATE, --lrate LRATE [0.0001]                             | l_rate (defined inside network structure)                                                            |
| Learning Rate for each layer              |      | —net_mode:  [128] turns on Adam optimization (better than plain momentum).  [64] enables automatic layer-specific learning rate. When progress stalls, the trainer investigates which layer(s) should have their learning rate reduced independently, and may lower one or more learning rates to continue learning. [192] enables both Adam and layer-specific learning rates. | ❌                                                            | ❌                                                            | ❌                                                            |
| Backend                                   |      |                                                              |                                                              |                                                              | —backend Tensorflow                                          |
| Random Seed                               |      |                                                              |                                                              |                                                              | —seed SEED, negative or 0, a random seed is used             |
| Learning Rate Scheduler                   |      | ❌                                                            | --schedule [constant: 1 cycle] cycle length decided by —epoch:  reduce the learning rate by a constant factor every few epochs | ❌                                                            | ❌                                                            |
| Momentum                                  |      | —momentum, double [0.5]                                      | -m, --momentum FLOAT [0.9]:Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. | ❌                                                            | ❌                                                            |
| Weight Decay                              |      | ❌                                                            | -w, --weight-decay FLOAT[0.0], penalize the large weight, similar to regularization | ❌                                                            | ❌                                                            |
| Ratio of Train/Valid                      |      | ❌                                                            | -p, --partition FLOAT[0.9]                                   | ❌                                                            | ❌                                                            |
| Validation Lines                          |      | —eval_listfile, str:  Filename of a file listing evaluation data files to be used in evaluating the model independently of the training data. | -e, --evaluation-files FILENAME, File(s) with paths to evaluation data. **Overrides the `-p` parameter **. | ❌                                                            | --validation VALIDATION: Validation line files used for **early stopping** |
| Optimizer                                 |      | —net_mode:[Momemtum], 128 Adam instead of Momentum.          | --optimizer [Adam] one from Adam, SGD, RMSprop               |                                                              | ❌                                                            |
| adam_beta                                 |      | —adam_beta[0.999] smoothing factor squared in Adam algorithm | ❌                                                            | ❌                                                            | ❌                                                            |
| batch size                                |      | ❌                                                            | ❌                                                            | ❌                                                            | --batch_size  BATCH_SIZE[1]                                  |
| Save Freq                                 |      | ❌                                                            | -F, --savefreq FLOAT [1.0] epochs                            | -F SAVEFREQ, --savefreq SAVEFREQ [1000] iterations:question: | --checkpoint_frequency CHECKPOINT_FREQUENCY[-1]: The frequency how often to write checkpoints during training. If 0 < value <= 1 the unit is in epochs,  thus relative to the number of training examples.**If  -1, the early_stopping_frequency will be used.** |
| \#lines before stop                       |      | max_iterations:question:       [0] (int)          Stop training after this many iterations. | -N, --epochs INTEGER[-1]:number of **epoches** to train      | -N NTRAIN, --ntrain NTRAIN 1,000,000]:number of **lines**    | --max_iters MAX_ITERS[1,000,000]:The number of **iterations** for training. If using early  stopping, this is the maximum number of iterations |
| Early stop                                |      | —target_error_rate[0.01] Stop training if the mean percent error rate gets below this value.(for train or test) | -q, --quit [early] set to early for early stopping, dumb for fixed number of epoches.<br>--min-delta[0.005] FLOAT: Minimum improvement between epochs to reset early stopping. Default is scales the delta by the best loss.<br>--lag int [5], number of evaluations to wait before stopping training without improvement.<br>-R, --report FLOAT[1.0] Report creation frequency in epochs |                                                              | --early_stopping_frequency :question:EARLY_STOPPING_FREQUENCY[0.5]:The frequency of early stopping. By default the checkpoint frequency uses the early stopping  frequency. By default (value = 0.5) the early stopping frequency equates to a half epoch. If 0 < value <= 1 the frequency has the unit of an epoch (relative to  number of training data).<br>--early_stopping_nbest EARLY_STOPPING_NBEST:The number of models that must be worse than the  current best model to stop<br>--early_stopping_best_model_prefix EARLY_STOPPING_BEST_MODEL_PREFIX:The prefix of the best model using early stopping<br>--early_stopping_best_model_output_dir EARLY_STOPPING_BEST_MODEL_OUTPUT_DIR: Path where to store the best model. Default is output_dir |
| Preload Data into Memory                  |      | ❌                                                            | --preload / --no-preload: Enables/disables preloading of the training set into memory for accelerated training. The default setting preloads data sets with less than 2500 lines, explicitly adding `--preload` will preload arbitrary sized sets. `--no-preload` disables preloading in all circumstances. | ❌                                                            | —train_data_on_the_fly: Instead of preloading all data during the training,  load the data on the fly. This is slower, but might be required for limited RAM or large datasets.                                                     —validation_data_on_the_fly:Instead of preloading all data during the training,  load the data on the fly. This is slower, but might be required for limited RAM or large datasets |
| Number of openMP threads                  |      | ❌                                                            | --threads INTEGER[1]: Number of OpenMP threads and workers when  running on CPU. | ❌                                                            | --num_threads NUM_THREADS:The number of threads to use for all operations.     —num_inter_threads,int, [0]"Tensorflow's session inter threads param")    --num_intra_threads, int, [0], Tensorflow's session intra threads param |
| Special                                   |      | --max_image_MB, int[6000], maximum amount of memory to use for caching images<br>--perfect_sample_delay,  int[0]: When the network gets good, only backprop a perfect sample after this many imperfect samples have been seen since the last perfect sample was allowed through.<br>--sequential_training:[bool, false], true for sequential training. Default to process all training data in round-robin fashion.<br>—traineddata[string,none]path to the starter trained data file that contains the unicharset, recorder and optional language model<br>—debug_interval[int,0]:If non-zero, show visual debugging every this many iterations. | -d, --device TEXT [cpu]:Select device to use (cpu, cuda:0, cuda:1, …)<br> | --start START[-1]:manually set the number of already learned lines, which influences the naming and stopping condition,  default: -1 which will then be overridden by the value  saved in the network:question: | —no_skip_invalid_gt, Do no skip invalid gt, instead raise an exception.<br>--gradient_clipping_mode GRADIENT_CLIPPING_MODE, Clipping mode of gradients. Defaults to AUTO, possible values are AUTO, NONE, CONSTANT. --gradient_clipping_const GRADIENT_CLIPPING_CONST:Clipping constant of gradients in CONSTANT mode.<br>--gt_extension GT_EXTENSION: Default extension of the gt files (expected to exist in same dir)<br> |
| Input                                     |      | —train_listfile(string, none) filename of a file listing training data files | -t, --training-files FILENAME:File(s) with additional paths to training data | -f FILE, --file FILE                                         | --files FILES [FILES …]: List all image files that shall be processed. Ground  truth files with the same base name but with '.gt.txt' as extension are required at the same location |
| Input format                              |      | .tif                                                         | *.png <br />*.gt.txt                                         | .png<br />.gt.txt                                            | .png<br />.gt.txt                                            |
| Output                                    |      | Prefix                                                       | Prefix                                                       | Prefix                                                       | Prefix/Directory                                             |
|                                           |      | —model_output[string, none]: Base path of output model files/checkpoints. | -o, --output PATH[model]:Output model file                   | -o OUTPUT, --output OUTPUT                                   | --output_model_prefix OUTPUT_MODEL_PREFIX<br>--early_stopping_best_model_prefix EARLY_STOPPING_BEST_MODEL_PREFIX<br>--early_stopping_best_model_output_dir EARLY_STOPPING_BEST_MODEL_OUTPUT_DIR |
| Best Model Path                           |      |                                                              | prefix in -o command  + '_best.mlmodel'                      |                                                              |                                                              |
| Train Command                             |      | training/lstmtraining --debug_interval 100   --traineddata ~/tesstutorial/engtrain/eng/eng.traineddata    --net_spec '[1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx256 O1c111]'   --model_output ~/tesstutorial/engoutput/base --learning_rate 20e-4 \   --train_listfile ~/tesstutorial/engtrain/eng.training_files.txt   --eval_listfile ~/tesstutorial/engeval/eng.training_files.txt   --max_iterations 5000 &>~/tesstutorial/engoutput/basetrain.log | ketos train output_dir/*.png                                 | ocropus-rtrain -o modelname book\*/????/\*.bin.png           | calamari-train --files your_images.*.png                     |
| Evaluation Command                        |      | tesseract imagename outputbase [-l lang] [-psm pagesegmode] [configfile…]<br>tesseract myscan.png out -l eng+deu | ketos test -m syriac_best.mlmodel lines/*.png                | ./ocropus-rpred -Q 4 -m models/fraktur.pyrnn.gz 'book/????/??????.bin.png' | calamari-predict --checkpoint path_to_model.ckpt --files your_images.*.png |
| Fine Tuning                               |      | training/lstmtraining --model_output /path/to/output [--max_image_MB 6000] \   --continue_from /path/to/existing/model \   --traineddata /path/to/original/traineddata \   [--perfect_sample_delay 0] [--debug_interval 0] \   [--max_iterations 0] [--target_error_rate 0.01] \   --train_listfile /path/to/list/of/filenames.txt | ketos train -i model_best.mlmodel syr/*.png                  |                                                              |                                                              |
